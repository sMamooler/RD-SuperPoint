{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from superpoint.settings import DATA_PATH, EXPER_PATH\n",
    "from superpoint.models.utils import detector_head, box_nms\n",
    "from superpoint.models.homographies import homography_adaptation\n",
    "from superpoint.radial_distortion.radial_dist_funct import distort\n",
    "from superpoint.models.backbones.vgg import vgg_backbone\n",
    "from superpoint.radial_distortion.radial_dist_funct import distort, undistort\n",
    "from superpoint.models.magic_point import MagicPoint\n",
    "from superpoint.models.base_model import BaseModel, Mode\n",
    "from utils import plot_imgs\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "tf.logging.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_point_verification(image, net):\n",
    "\n",
    "    image_shape = tf.shape(image)\n",
    "    H = image_shape[1]\n",
    "    W = image_shape[2]\n",
    "    row_c = tf.random.uniform(shape=[], minval=0, maxval=tf.cast(H, tf.float32), dtype=tf.float32)\n",
    "    col_c = tf.random.uniform(shape=[], minval=0, maxval=tf.cast(W, tf.float32), dtype=tf.float32) \n",
    "    lambda_ = tf.constant(0.00001)\n",
    "\n",
    "    d_image = distort(image,lambda_,(row_c,col_c))\n",
    "    \n",
    "    \n",
    "    u_pred = net(image)\n",
    "    d_pred = net(d_image)\n",
    "\n",
    "    u_pred['prob_nms'] = box_nms(tf.reshape(u_pred['prob'], [240,320]), net_config['nms'], keep_top_k=net_config['top_k'])\n",
    "    u_pred['pred'] = tf.to_int32(tf.greater_equal(u_pred['prob_nms'], net_config['detection_threshold']))\n",
    "\n",
    "    d_pred['prob_nms'] = box_nms(tf.reshape(d_pred['prob'], [240,320]), net_config['nms'], keep_top_k=net_config['top_k'])\n",
    "    d_pred['pred'] = tf.to_int32(tf.greater_equal(d_pred['prob_nms'], net_config['detection_threshold']))\n",
    "\n",
    "\n",
    "    return {'u_pred':u_pred['pred'], 'd_pred':d_pred['pred'],'dist_image': d_image, 'lambda': lambda_, 'center': (row_c, col_c)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_config = {\n",
    "    'data_format': 'channels_last',\n",
    "    'grid_size': 8,\n",
    "    'detection_threshold': 0.001,\n",
    "    'nms': 4,\n",
    "    'descriptor_size': 256,\n",
    "    'top_k': 300,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.01,\n",
    "    'homography_adaptation': {\n",
    "        'num': 0, \n",
    "        'aggregation': 'sum',\n",
    "        'kernel_reg': 0.,\n",
    "        'homographies': {\n",
    "            'translation': True,\n",
    "            'rotation': True,\n",
    "            'scaling': True,\n",
    "            'perspective': True,\n",
    "            'scaling_amplitude': 0.1,\n",
    "            'perspective_amplitude_x': 0.22,\n",
    "            'perspective_amplitude_y': 0.22,\n",
    "            'translation_overflow': 0.1,\n",
    "            'allow_artifacts': True\n",
    "        },\n",
    "        'filter_counts': 2,\n",
    "    },\n",
    "    'training': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(image):\n",
    "    with tf.variable_scope('magicpoint', reuse=tf.AUTO_REUSE):\n",
    "        image = tf.reshape(image, [1,240, 320,1])\n",
    "        features = vgg_backbone(image, **net_config)\n",
    "        return detector_head(features, **net_config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_keypoints(img, corners, color, s = 4):\n",
    "    keypoints = [cv2.KeyPoint(c[1]*s, c[0]*s, 1) for c in np.stack(corners).T]\n",
    "    img = cv2.resize(img, None, fx=s, fy=s)\n",
    "    return cv2.drawKeypoints(img.astype(np.uint8), keypoints, None, color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 'COCO_train2014_000000519723.jpg' #'COCO_train2014_000000130745.jpg' #'COCO_train2014_000000000071.jpg' #'COCO_train2014_000000114404.jpg' #'COCO_train2014_000000151493.jpg' #\n",
    "model_name = 'magic-point_synth'\n",
    "base_path = Path(DATA_PATH, 'COCO/train2014/')\n",
    "filename = Path(base_path, image_name)\n",
    "checkpoint = Path(EXPER_PATH, model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"magicpoint/Shape:0\", shape=(4,), dtype=int32)\n",
      "Tensor(\"magicpoint_1/Shape:0\", shape=(4,), dtype=int32)\n",
      "Device mapping:\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3688: error: (-215:Assertion failed) !dsize.empty() in function 'cv::hal::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-c3fc76b63b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mim_u\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_keypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mim_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_keypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_image\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mim_d_undist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_keypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_pred_undistorted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-6e633e6028a2>\u001b[0m in \u001b[0;36mdraw_keypoints\u001b[1;34m(img, corners, color, s)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_keypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mkeypoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawKeypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3688: error: (-215:Assertion failed) !dsize.empty() in function 'cv::hal::resize'\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph(),\n",
    "image = tf.image.decode_jpeg(tf.read_file(str(filename)), channels=1)\n",
    "image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "image = tf.to_float(tf.image.resize_images(image, [240, 320]))\n",
    "\n",
    "outputs = key_point_verification(image[tf.newaxis,...], net)\n",
    "\n",
    "#checkpoint = 'magic-point_synth'\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,  log_device_placement=True)) as sess:\n",
    "\n",
    "    tf.train.Saver().restore(sess, tf.train.latest_checkpoint(checkpoint));\n",
    "    im, _out = sess.run([image,outputs])\n",
    "    \n",
    "    d_image = _out['dist_image']\n",
    "    u_pred =  np.reshape(_out['u_pred'], [240, 320])\n",
    "    d_pred = np.reshape(_out['d_pred'], [240, 320])\n",
    "    row_c, col_c = _out['center']\n",
    "    lambda_ = _out['lambda']\n",
    "    \n",
    "    arr_col = np.where(d_pred)[0]-[col_c]\n",
    "    arr_row = np.where(d_pred)[1]-[row_c]\n",
    "    delta = np.transpose((arr_row,arr_col))\n",
    "    norms = np.linalg.norm(delta, None,1 ,keepdims=True)**2\n",
    "    denom = lambda_ * norms + 1\n",
    "    d_pred_undistorted = (delta / denom + [row_c,col_c])\n",
    "    #d_pred_undistorted = [d_pred_undistorted[1],d_pred_undistorted[0]]\n",
    "   \n",
    "\n",
    "\n",
    "    im_u = draw_keypoints(im[..., 0]*255, np.where(u_pred), (0, 255, 0))/255.\n",
    "    im_d = draw_keypoints(d_image*255, np.where(d_pred), (0, 255, 0))/255.\n",
    "    im_d_undist = draw_keypoints(im[..., 0]*255, d_pred_undistorted, (0, 255, 0))/255.\n",
    "\n",
    "    #This is the keypoints of the original image\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1,1,1)\n",
    "    plt.imshow(im_u)\n",
    "\n",
    "    #This is what superpoint gives for the distorted image\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1,1,1)\n",
    "    plt.imshow(im_d)\n",
    "    \n",
    "    #If superpoint is robust to radial dist, this should give the same result as the first plot\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1,1,1)\n",
    "    plt.imshow(im_d_undist)\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
